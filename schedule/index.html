<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Schedule | distribits</title><meta name=keywords content><meta name=description content="Important dates Registration opens: September 14th Registration and submission closes: midnight October 22nd Registration and submission feedback: November 1st Late Registration opens, with hackathon waiting list: November 6th At a glance Conference Thursday, 4 April 2024, 9&ndash;17 Friday, 5 April 2024, 9&ndash;17 Hackathon Saturday, 6 April 2024, 9&ndash;17 Event schedule (preliminary) You can access the (preliminary) raw schedule data as Pentabarf XML.
For viewing the schedule on mobile, we recommend the Giggity app (F-Droid, Google Play) &ndash; enter the link to the schedule file inside the app."><meta name=author content><link rel=canonical href=https://distribits.live/schedule/><link crossorigin=anonymous href=/assets/css/stylesheet.2df39c736ee41831a7adee7b150b8f0d1fa3b99b908c068b5a437ff64dc66d56.css integrity="sha256-LfOcc27kGDGnre57FQuPDR+juZuQjAaLWkN/9k3GbVY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://distribits.live/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://distribits.live/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://distribits.live/favicon-32x32.png><link rel=apple-touch-icon href=https://distribits.live/apple-touch-icon.png><link rel=mask-icon href=https://distribits.live/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Schedule"><meta property="og:description" content="Important dates Registration opens: September 14th Registration and submission closes: midnight October 22nd Registration and submission feedback: November 1st Late Registration opens, with hackathon waiting list: November 6th At a glance Conference Thursday, 4 April 2024, 9&ndash;17 Friday, 5 April 2024, 9&ndash;17 Hackathon Saturday, 6 April 2024, 9&ndash;17 Event schedule (preliminary) You can access the (preliminary) raw schedule data as Pentabarf XML.
For viewing the schedule on mobile, we recommend the Giggity app (F-Droid, Google Play) &ndash; enter the link to the schedule file inside the app."><meta property="og:type" content="article"><meta property="og:url" content="https://distribits.live/schedule/"><meta property="article:section" content><meta name=twitter:card content="summary"><meta name=twitter:title content="Schedule"><meta name=twitter:description content="Important dates Registration opens: September 14th Registration and submission closes: midnight October 22nd Registration and submission feedback: November 1st Late Registration opens, with hackathon waiting list: November 6th At a glance Conference Thursday, 4 April 2024, 9&ndash;17 Friday, 5 April 2024, 9&ndash;17 Hackathon Saturday, 6 April 2024, 9&ndash;17 Event schedule (preliminary) You can access the (preliminary) raw schedule data as Pentabarf XML.
For viewing the schedule on mobile, we recommend the Giggity app (F-Droid, Google Play) &ndash; enter the link to the schedule file inside the app."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Schedule","item":"https://distribits.live/schedule/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Schedule","name":"Schedule","description":"Important dates Registration opens: September 14th Registration and submission closes: midnight October 22nd Registration and submission feedback: November 1st Late Registration opens, with hackathon waiting list: November 6th At a glance Conference Thursday, 4 April 2024, 9\u0026ndash;17 Friday, 5 April 2024, 9\u0026ndash;17 Hackathon Saturday, 6 April 2024, 9\u0026ndash;17 Event schedule (preliminary) You can access the (preliminary) raw schedule data as Pentabarf XML.\nFor viewing the schedule on mobile, we recommend the Giggity app (F-Droid, Google Play) \u0026ndash; enter the link to the schedule file inside the app.","keywords":[],"articleBody":"Important dates Registration opens: September 14th Registration and submission closes: midnight October 22nd Registration and submission feedback: November 1st Late Registration opens, with hackathon waiting list: November 6th At a glance Conference Thursday, 4 April 2024, 9–17 Friday, 5 April 2024, 9–17 Hackathon Saturday, 6 April 2024, 9–17 Event schedule (preliminary) You can access the (preliminary) raw schedule data as Pentabarf XML.\nFor viewing the schedule on mobile, we recommend the Giggity app (F-Droid, Google Play) – enter the link to the schedule file inside the app.\n2024-04-04 Arrival and registration 08:30 (00:30) Foyer Welcome and overview 09:00 (00:30) Event Hall Abstract Welcome from the organizers\n\"What's in the DataLad sandwich\" AKA DataLad \"ecosystem\" 09:30 (00:20) Event Hall Yaroslav Halchenko Abstract At the heart of many innovative tools lies a simple spark of necessity. For DataLad, that spark was a father's quest in 2013 for an effortless way to access free children's cartoons and movies. What started to scratch a personal itch, has evolved into a grant funded DataLad platform addressing a broad range of data logistics challenges. Utilizing the strengths of git and git-annex, DataLad has not only expanded its capabilities but has also contributed to the enhancement of git-annex features, tailor-made to suit its needs. Through the innovative use of git external protocols and git-annex external special remotes, DataLad offers a seamless experience to users, fetching data with remarkable flexibility. To push the boundaries further, DataLad introduced an \"extensions mechanism,\" enabling the platform to adapt and extend beyond its core functionalities. This modular architecture, while offering unparalleled flexibility, hints at a potential for complexity and fragility. In this presentation, I will take you on a journey through the foundational elements that give DataLad its unique extensibility—spanning git, git-annex, and beyond—with few practical examples that bring these concepts to life. Despite the inherent challenges of a modular system, our dedicated \"dev-ops\" components, which I will demonstrate, ensure a stable and efficient ecosystem. By developing, testing, and distributing these components, we've crafted not just a tool, but a robust platform ready to tackle the data logistics needs of today and tomorrow.\n\"git annex is complete, right?\" 09:50 (00:20) Event Hall Joey Hess Abstract My father has asked me this question before over the years. So has an experienced developer recently. Seeing the same question from two such different perspectives got me asking it of myself. While a new data storage system can always be added to git-annex, or a new command be added to improve some use case, both of those can also be accomplished without needing changes to git-annex, by external remotes and more targeted frontends such as DataLad. So what then is the potential surface area of problem space that git-annex may expand to cover? Do diminishing returns and complexity make such expansions a good idea? I will explore this by considering recent developments in git-annex, and the impact of lesser-used features.\nDataLad beyond Git, connecting to the rest of the world 10:10 (00:20) Event Hall Michael Hanke Abstract DataLad has been built on Git and git-annex as foundational pillars. However, the vast majority of data infrastructures are not Git-aware. Git-annex can work with a much broader array of services, but the need to \"keep the Git repo somewhere\" imposes undesirable technical and procedural complexity on users. In this talk I illustrate existing means to take Git-based DataLad datasets to places that Git cannot reach on its own. Moreover, I introduce ongoing work that aims to enable DataLad users to consume non-DataLad resources as native DataLad datasets, and non-DataLad users to consume DataLad resources without DataLad, git-annex, or even Git.\nQuestions and panel discussion 10:30 (00:30) Event Hall Abstract Questions and panel discussion\nCoffee 11:00 (00:30) Foyer Git annex recipes 11:30 (00:20) Event Hall Timothy Sanders Abstract I have come up with many recipes over the years for scaling git-annex repositories in terms of large numbers of keys, large file sizes, and increased transfer efficiency. I have working examples that I use internally that I can demonstrate. (1) Second-order keys; using metadata to describe keys that can be derived from other keys. I primarily used this to help with the problem of too many keys referencing small files. This is building off of the work of others, but I believe I have made useful improvements, and I would like to polish it up and share it. One very early example is here: https://github.com/unqueued/repo.macintoshgarden.org-fileset/ For now, I stripped out all but the location data from the git-annex branch. Files smaller than 50M are contained in second-order keys (8b0b0af8-5e76-449c-b0ae-766fcac0bc58). The other uuids are for standard backends, including a Google Drive account which has very strict limits on requests, and it would have been very difficult to process over 10k keys directly. There are also other cases where keys can be reliably reproduced from other keys. (2) Differential storage with git-annex using bup (or borg). I built of off posts on the forums from years ago, and came up with some really useful workflows for combining the benefits of git-annex location tracking and branching with differential compression. I have scripts used for automation, and some example repos and case studies. For example, I have a repo which contains file indexes that are over 60GiB, but only consume about 6GiB, using bup packfiles. I can benefit from differential compression over different time ranges, like per year, or for the entire history, while minimizing storage usage. I will publish a working example in the next few weeks, but I have only used it internally for years.\nOpenNeuro and DataLad 11:50 (00:20) Event Hall Nell Hardcastle Abstract A history of OpenNeuro's adoption of DataLad and the evolution of DataLad and git-annex support on the platform. In 2017 OpenNeuro was preparing to launch with the original data backend implemented as block storage without git-annex. The decision was made to move OpenNeuro to DataLad and a quick prototype for this backend service was created and eventually brought to production for the public release of OpenNeuro. Since 2017 the platform has evolved to support many of the unique advantages of DataLad datasets. This talk discusses the architecture of OpenNeuro, some of the challenges encountered using git-annex as the center of our application’s data model in cloud environments, solutions developed, and future work to improve upon OpenNeuro’s archival and distribution of DataLad datasets.\nQuestions and panel discussion 12:10 (00:20) Event Hall Abstract Questions and panel discussion\nLunch (self-organized, outside venue) 12:30 (01:30) Foyer GIN-Tonic: (trying to) making datalad accessible to non-coders 14:00 (00:20) Event Hall Julien Colomb Abstract With the GIN-Tonic suite, we work toward a workflow where the users only needs to use the browser and some synchronisation scripts to manage their datalad repositories. Settings are prepared in a template (including submodules) and a browser extension (written in Go) create new repositories from that template. I would report on successes and failures of the strategy.\nOnedata as a Platform: Distributed Repository for Virtualizing Data and Long-term Preservation 14:20 (00:20) Event Hall Łukasz Dutka Abstract With the proliferation of digital data, reliable storage, easy accessibility, and long-term preservation have become paramount. Onedata, a novel platform, emerges as a solution for these challenges by enabling a distributed repository framework for virtualizing data. This presentation delves into how Onedata facilitates seamless data management and ensures long-term preservation. By virtualizing data, Onedata abstracts the underlying storage infrastructures enabling a unified view and easy sharing among different stakeholders. Furthermore, its distributed repository nature significantly enhances data durability and availability. The in-built mechanisms for metadata management and data replication ensure that the information remains intact and accessible over extended periods. Through a detailed exploration of its architecture and functionalities, this presentation will highlight how Onedata can be a robust platform for modern data management and long-term preservation needs, catering to academia, industry, and beyond. The insights provided will foster a better understanding of leveraging distributed repository platforms in navigating the complex landscape of digital data preservation.\nQuestions and panel discussion 14:40 (00:20) Event Hall Abstract Questions and panel discussion\nCoffee 15:00 (00:30) Foyer Workflow provenance–based scheduling 15:30 (00:20) Event Hall Pedro Martinez Abstract Scientific computing workflows have become increasingly complex, often comprising of numerous interdependent tasks executed on distributed computing resources. Provenance data, or the history of computational processes, provide a vital link between data reproducibility and task scheduling. Workflows with recorded data provenance can seamlessly integrate with separate workflow management systems, eliminating the need for inter-system communication. In this talk, we introduce a novel tool to perform provenance-based workflow scheduling. Our approach leverages an abstract graph builder tool designed to create abstract graphs representing the high-level structure of workflows. These abstract graphs emphasize dependencies and data flows, facilitating a better understanding of the computational process. Concurrently, we extract concrete graphs from workflow provenance data recorded with DataLad that reflect the actual execution history. The core of our approach lies in comparing the abstract graph to concrete graphs produced by separate runs of the workflow for a set of input parameters. By computing the difference we can pinpoint tasks that remain unexecuted or require re-execution due to errors or changes in input data and automatically schedule these tasks. We will outline future directions for this research, including potential extensions to support system agnostic scheduling, and scalability considerations.\nOptimisation in Network Engineering: Challenges and Solutions in Research Data Management 15:50 (00:10) Event Hall Julius Breuer Abstract In the complex realm of network engineering design, optimisation methods have been instrumental, using a range of components across different systems and scenarios. However, this complexity presents a dual challenge: first, managing, tracking and combining thousands of optimisation calculations, including the specifics of component data, system classifications, scenarios considered, and settings applied. Second, integrating diverse data from multiple sources that do not all reside in one place. Third, the possibility of collaboration (in this case with students, potentially with more people). Such challenges emphasise the need for rigorous research data management. Questions such as \"which component data was used in which system?\" or the provenance of component data come to the fore. To answer these questions, DataLad is used to store disparate data, models, settings and results in an effective and distributed manner. DataLad's provenance reduces the redundancy of storage and the effort required for publication, while increasing confidence in the results. This is done in the context of a research project, but the same questions arise for the industrial application of what has been researched.\nfMRI Pipelines on HPC with DataLad and ReproMan 16:00 (00:10) Event Hall Joe Wechsler Abstract In this lightning talk, I will share my experience using DataLad, git-annex and ReproMan to run software pipelines on hundreds of fMRI datasets on an HPC cluster. Potential topics may include: (a) The use of ReproMan to avoid the difficulties of using datalad containers-run in parallel on an HPC. (b) How to use DataLad on a scratch filesystem that periodically purges files to save space. (c) A simple algorithm I implemented in ReproMan to prevent excess runtime due to outliers in parallel jobs. (d) The pros and cons of the YODA-BIDS layout for neuroimaging data. I hope my talk will prompt discussion with those hoping to learn more from my experience as well as those who have found alternative solutions to similar challenges.\nReproducibility vs. computational efficiency on HPC systems 16:10 (00:10) Event Hall Felix Hoffstaedter Abstract HPC systems have particular hard- and software configurations that introduce specific challenges for the implementation of reproducible data processing workflows. The DataLad based 'FAIRly big workflow' allows for a separation of the compute environment from the processing pipeline enabling automatic reproducibility over systems. Yet, the sheer size of RAM and CPUs on HPC systems will allow for different ways to optimize compute jobs in contrast to compute clusters and certainly the average workstation/laptop. In this talk, I discuss general differences between HCP and more standard compute environments regarding necessary choices for the setup of processing pipelines to be reproducible. Among the main factors are the availability of RAM, local storage, inodes and wall clock time.\nQuestions and panel discussion 16:20 (00:40) Event Hall Abstract Questions and panel discussion\nEnd of day 17:00 (00:30) Foyer 2024-04-05 Coffee 08:30 (00:30) Foyer Welcome and overview 09:00 (00:15) Event Hall Abstract Welcome and overview: day 2.\nNeuroscientific data management using DataLad 09:15 (00:20) Event Hall Julian Kosciessa Abstract Robust data management from raw data to result publication is necessary to make scientific research more widely reusable. This remains a challenge, particularly in projects that involve a variety of subcomponents and large data. In this talk, I provide a user perspective on using DataLad procedures for structuring, managing, and sharing complex cognitive neuroscience projects. By showcasing example multimodal neuroimaging projects that include e.g., electroencephalography (EEG), functional magnetic resonance imaging (fMRI), and behavioral data, I will highlight workflows that are uniquely enabled by the distributed nature of DataLad. Based on my experiences, I will also indicate remaining roadblocks I perceive to widespread adoption.\nStaying in Control of your Scientific Data with Git Annex 09:35 (00:20) Event Hall Yann Büchau Abstract Scientific experiments can produce a lot of data, often very different in kind and scattered across devices and even remote locations. Keeping all of this in check is not a simple task and failure to do so can easily cause data loss due to accidental deletion or hardware failure (think cheap SD cards in measurement devices at remote locations). Git Annex can help with synchronisation, catalogisation, versioning and archival of data as well as collaboration.\nQuestions and panel discussion 09:55 (00:20) Event Hall Abstract Questions and panel discussion\nFusion of Multiple Open Source Applications for Data Management Workflows in Psychology and Neuroscience 10:15 (00:20) Event Hall Julia-Katharina Pfarr Abstract Finding a compromise between researchers’ needs, their skills in data management, data access restrictions, and limited funding for RDM is a complex but highly relevant and timely challenge. At the University of Marburg this challenge is taken on by the team of the “Data Hub”. The team consists of several people with different responsibilities, backgrounds, and affiliations such as project management staff, scientific staff, data stewards, data scientists, technical administrative staff, located in Marburg and Gießen. The Data Hub is funded by The Adaptive Mind (TAM) and supported by the information infrastructure project (NOWA) of the SFB135, which are consortia in the fields of psychology and neuroscience, with over 50 involved PIs, based in several locations in the federal state of Hesse, Germany. Although the research data in the two consortia are restricted to the fields of psychology and neuroscience, a major challenge is the need to harmonize heterogeneous data. The data encompass research data from different modalities such as behavior, eye tracking, EEG and neuroimaging as well as code for experiments and analysis in various programming languages. Therefore, the data management workflow needs to be applicable to heterogeneous in- and output data, different project sizes, and numbers of researchers involved. Furthermore, tools need to be able to integrate those heterogeneous data by utilizing a harmonizing standard in the field (here: BIDS). To increase the reproducibility of research findings, an integration of version control and provenance tracking (here: DataLad) should be available. For this, the team must have an understanding at which point to include the researchers: How much background knowledge about software do they have and how much do they really need? Which functions of the software are necessary and which ones can be skipped because they’ll never apply to the researchers’ work? Do they need a lot of hands-on practice or is the concept enough? In our presentation, we will first introduce the Data Hub of the University of Marburg and its technical architecture. We will then present the data management tools utilized in the Data Hub, i.e., DataLad, GIN, GitLab, JupyterHub, and BIDS. We will specifically focus on how these tools are interconnected, i.e., the research data management workflow of the Data Hub. Then, we will outline the challenges for both the researchers as well as the Data Stewards regarding training, support and maintenance of the services.\nBalancing Efficiency and Standardization for a Microscopic Image Repository on an HPC System 10:35 (00:20) Event Hall Julia Thönnißen Abstract Understanding the human brain is one of the greatest challenges of modern science. In order to study its complex structural and functional organization, data from different modalities and resolutions must be linked together. This requires scalable and reproducible workflows ranging from the extraction of multimodal data from different repositories to AI-driven analysis and visualization [1]. One fundamental challenge therein is to store and organize big image datasets in appropriate repositories. Here we address the case of building a repository of high-resolution microscopy scans for whole human brain sections in the order of multiple Petabytes [1]. Since data duplication is prohibitive for such volumes, images need to be stored in a way that follows community standards, supports provenance tracking, and meets performance requirements of high-throughput ingestion, highly parallel processing on HPC systems, as well as ad-hoc random access for interactive visualization. To digitize an entire human brain, high-throughput scanners need to capture over 7000 histological brain sections. During this process, a scanner acquires a z-stack, which consists of 30 TIFF images per tissue section, each representing a different focus level. The images are automatically transferred from the scanner to a gateway server, where they are pre-organised into subfolders per brain section for detailed automated quality control (QC). Once a z-stack passes QC, it is transferred to the parallel file system (GPFS) on the supercomputer via NFS-mount. For one human brain, this results in 7000 folders with about 2 PByte of image data in about 20K files in total. From there, the data are accessed simultaneously by different applications and pipelines with their very heterogeneous requirements. HPC analyses based on Deep Learning such as cell segmentation or brain mapping rely on fast random access and parallel I/O to stream image patches efficiently to GPUs. Remote visualization and annotation on the other hand requires exposure of the data through an HTTP service on a VM, with access to higher capacity storage to serve different data at the same time. These demands can be covered by multi-tier HPC storage, which provides dedicated partitions. The High Performance Storage Tier offers low latency and high bandwidth for analysis, while the Extended Capacity Storage Tier is capacity-optimized with a lower latency, meeting the needs for visualization. Exposing the data on different tiers requires controlled staging and unstaging. We organize the image data folders via DataLad datasets, which allows well defined staging across these partitions for different applications, ensures that all data is tracked and versioned from distributed storage throughout the workflow, and enables provenance tracking. To reduce the number of files in one DataLad repository, each section folder has been designed as a subdataset of a superdataset that contains all section folders. The current approach to managing data has two deficiencies. Firstly, the TIFF format is not optimized for HPC usage due to the lack of parallel I/O support, resulting in data duplication due to conversion to HDF5. Secondly, the current data organization is not compatible with upcoming community standards, complicating collaborative efforts. Therefore, standardization of the file format and folder structure is a major objective for the near future. The widely accepted community standard for organizing neuroscience data is the Brain Imaging Data Structure (BIDS). Its extension for microscopy proposes splitting the data into subjects and samples, while using either (OME-)TIFF or OME-ZARR as a file format. Particularly, the NGFF file format OME-ZARR appears to be the suitable choice for the workflow described, as it is more performant on HPC and cloud compatible as opposed to TIFF. However, restructuring the current data layout is a complex task. Adopting the BIDS standard results in large amounts of inodes and files because (1) multiple folders and sidecar files are created and (2) OME-ZARR files are comprised of many small files. DataLad annex undergoes expansion with the increase in the number of files leading to high inode usage and reduced performance. An effective solution to this problem may involve the optimization of the size of DataLad subdatasets. However, the key consideration is that GPFS file systems enforce a limit on the number of inodes, which cannot be surpassed. This raises the following questions: How can usage of inodes be minimized while adhering to BIDS and utilizing DataLad? Should performant file formats with minimal inode usage, such as ZARR v3 or HDF5, be incorporated into the BIDS standard? What is a good balance for DataLad subdataset sizes? Discussions with the community may provide valuable perspectives for advancing this issue. [1] Amunts K, Lippert T. Brain research challenges supercomputing. Science 374, 1054-1055 (2021). DOI:10.1126/science.abl8519\nDataLad-Registry: Bringing Benefits of Centrality to DataLad 10:55 (00:20) Event Hall Isaac To, Yaroslav Halchenko Abstract DataLad-Registry is a service that maintains up-to-date information on over ten thousand datasets, with the collection expanding as more datasets are added. This talk will explore how DataLad-Registry automatically registers datasets from the internet, extracts metadata from them, and keeps these datasets and their corresponding metadata up-to-date. We'll showcase the datasets and metadata types currently available within DataLad-Registry and demonstrate the service's search capability. Additionally, we'll provide an overview of the API and reveal the underlying service components of DataLad-Registry. The presentation will conclude with a discussion on ongoing and future developments, inviting audience input to shape the future of DataLad-Registry.\nQuestions and panel discussion 11:15 (00:45) Event Hall Abstract Questions and panel discussion\nLunch (self-organized, outside venue) 12:00 (01:30) Foyer Reproducible and replicable data science in the presence of patient confidentiality concerns by utilizing git-annex and the Data Science Orchestrator 13:50 (00:40) Event Hall Markus Katharina Brechtel, Philipp Kaluza Abstract Health-related data for patients is among the most sensitive data when it comes to data privacy concerns. Data science projects in the medical domain must thus pass a very high bar before allowing data researchers access to potentially personally identifiable data, or pseudonymized patient data that carries an inherent risk of depseudonymization. In the project \"Data Science Orchestrator\", we propose an organizational framework for ethically chaperoning and risk-managing such projects while they are under way, and a software stack that helps in this task. At the same time this software stack will provide an audit trail across the project that is verifyable even by external scientists without access to the raw data, while keeping the option for future reproducibility studies and replicability studies open. This is achieved by utilizing git-annex and datalad in a novel way to provide partial data blinding. Because collecting study-relevant data is often a time- and labor-intensive undertaking in the medical domain, many projects are undertaken by associations that span multiple hospitals, administrative domains, and often even multiple states. Therefore the \"Data Science Orchestrator\" project also implements distributed data science computations, which allow to honor these existing administrative boundaries by means of a federated access model, all while keeping the most sensitive data in-house and exclusively in a tightly controlled computation environment. This work was sponsored by Deutsche Zentren für Gesundheitsforschung (DZG) and BMBF.\nA Tour of Magit 14:30 (00:20) Event Hall Kyle Meyer Abstract Magit is an Emacs interface to Git. Through it, you can drive Git operations, even advanced ones, by typing short key sequences. This talk will show Magit in action. I will give a general overview and then highlight features for preparing and refining a series of commits.\nQuestions and panel discussion 14:50 (00:20) Event Hall Abstract Questions and panel discussion\nCoffee 15:10 (00:30) Foyer Distributed Metadata and Data with Dataverse 15:40 (00:40) Event Hall Philip Durbin, Jan Range, Oliver Bertuch Abstract Dataverse is open source research data repository software that has supported distributed metadata for a long time and is increasingly supporting distributed data. Learn about the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) and file \"stores\" within Dataverse that can be hosted locally, on S3, Globus, or remote locations. We plan to demonstrate a proof of concept for a distributed storage configuration in Jülich DATA and the use of Dataverse APIs to manage data with Python.\nUnconference 16:20 (00:40) Event Hall Abstract Unconference slot\nDinner and social (self-organized, outside venue) 17:00 (01:00) Foyer 2024-04-06 Coffee 08:30 (00:30) Seminar room 1, 3rd floor Kick off / Pitches 09:00 (00:30) Seminar room 1, 3rd floor Hacking 09:30 (02:30) Seminar room 1, 3rd floor Lunch (self-organized, outside venue) 12:00 (01:30) Seminar room 1, 3rd floor Hacking 13:30 (01:00) Seminar room 1, 3rd floor Coffee 14:30 (00:15) Seminar room 1, 3rd floor Hacking 14:45 (01:45) Seminar room 1, 3rd floor Wrap-up 16:30 (00:30) Seminar room 1, 3rd floor Dinner and social (self-organized, outside venue) 17:00 (02:00) Seminar room 1, 3rd floor ","wordCount":"4111","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://distribits.live/schedule/"},"publisher":{"@type":"Organization","name":"distribits","logo":{"@type":"ImageObject","url":"https://distribits.live/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://distribits.live accesskey=h title="distribits (Alt + H)">distribits</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://distribits.live/about/ title=About><span>About</span></a></li><li><a href=https://distribits.live/location/ title=Location><span>Location</span></a></li><li><a href=https://distribits.live/schedule/ title=Schedule><span class=active>Schedule</span></a></li><li><a href=https://distribits.live/news/ title=News><span>News</span></a></li><li><a href=https://distribits.live/code-of-conduct/ title="Code of Conduct"><span>Code of Conduct</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Schedule</h1><div class=post-meta></div></header><div class=post-content><h2 id=important-dates>Important dates<a hidden class=anchor aria-hidden=true href=#important-dates>#</a></h2><ul><li><em>Registration opens: September 14th</em></li><li><em>Registration and submission closes: midnight October 22nd</em></li><li><em>Registration and submission feedback: November 1st</em></li><li><strong>Late Registration opens, with hackathon waiting list: November 6th</strong></li></ul><h2 id=at-a-glance>At a glance<a hidden class=anchor aria-hidden=true href=#at-a-glance>#</a></h2><ul><li>Conference<ul><li><a href=#2024-04-04>Thursday, 4 April 2024, 9&ndash;17</a></li><li><a href=#2024-04-05>Friday, 5 April 2024, 9&ndash;17</a></li></ul></li><li>Hackathon<ul><li><a href=#2024-04-06>Saturday, 6 April 2024, 9&ndash;17</a></li></ul></li></ul><h2 id=event-schedule-preliminary>Event schedule (preliminary)<a hidden class=anchor aria-hidden=true href=#event-schedule-preliminary>#</a></h2><p>You can access the (preliminary) raw schedule data as Pentabarf <a href=/sched/distribits2024.xml>XML</a>.</p><p>For viewing the schedule on mobile, we recommend the Giggity app (<a href=https://f-droid.org/packages/net.gaast.giggity/>F-Droid</a>, <a href="https://play.google.com/store/apps/details?id=net.gaast.giggity">Google Play</a>) &ndash; enter the link to the schedule file inside the app.</p><h2 id=2024-04-04>2024-04-04</h2><h3>Arrival and registration</h3><p>08:30 (00:30) Foyer</p><p></p><h3>Welcome and overview</h3><p>09:00 (00:30) Event Hall</p><p></p><details><summary>Abstract</summary><p>Welcome from the organizers</p></details><h3>"What's in the DataLad sandwich" AKA DataLad "ecosystem"</h3><p>09:30 (00:20) Event Hall</p><p>Yaroslav Halchenko</p><details><summary>Abstract</summary><p>At the heart of many innovative tools lies a simple spark of
necessity. For DataLad, that spark was a father's quest in
2013 for an effortless way to access free children's
cartoons and movies. What started to scratch a
personal itch, has evolved into a grant funded DataLad
platform addressing a broad range of data logistics
challenges. Utilizing the strengths of git and git-annex,
DataLad has not only expanded its capabilities but has also
contributed to the enhancement of git-annex features,
tailor-made to suit its needs. Through the innovative use of
git external protocols and git-annex external special
remotes, DataLad offers a seamless experience to users,
fetching data with remarkable flexibility.
To push the boundaries further, DataLad introduced an
"extensions mechanism," enabling the platform to adapt and
extend beyond its core functionalities. This modular
architecture, while offering unparalleled flexibility, hints
at a potential for complexity and fragility.
In this presentation, I will take you on a journey through
the foundational elements that give DataLad its unique
extensibility—spanning git, git-annex, and beyond—with few
practical examples that bring these concepts to
life. Despite the inherent challenges of a modular system,
our dedicated "dev-ops" components, which I will
demonstrate, ensure a stable and efficient ecosystem. By
developing, testing, and distributing these components,
we've crafted not just a tool, but a robust platform ready
to tackle the data logistics needs of today and tomorrow.</p></details><h3>"git annex is complete, right?"</h3><p>09:50 (00:20) Event Hall</p><p>Joey Hess</p><details><summary>Abstract</summary><p>My father has asked me this question before over the years. So
has an experienced developer recently. Seeing the same
question from two such different perspectives got me asking it
of myself.
While a new data storage system can always be added to
git-annex, or a new command be added to improve some use case,
both of those can also be accomplished without needing changes
to git-annex, by external remotes and more targeted frontends
such as DataLad.
So what then is the potential surface area of problem space
that git-annex may expand to cover? Do diminishing returns and
complexity make such expansions a good idea? I will explore
this by considering recent developments in git-annex, and the
impact of lesser-used features.</p></details><h3>DataLad beyond Git, connecting to the rest of the world</h3><p>10:10 (00:20) Event Hall</p><p>Michael Hanke</p><details><summary>Abstract</summary><p>DataLad has been built on Git and git-annex as foundational pillars.
However, the vast majority of data infrastructures are not Git-aware.
Git-annex can work with a much broader array of services,
but the need to "keep the Git repo somewhere" imposes undesirable
technical and procedural complexity on users.
In this talk I illustrate existing means to take Git-based DataLad
datasets to places that Git cannot reach on its own.
Moreover, I introduce ongoing work that aims to enable DataLad
users to consume non-DataLad resources as native DataLad datasets,
and non-DataLad users to consume DataLad resources without DataLad,
git-annex, or even Git.</p></details><h3>Questions and panel discussion</h3><p>10:30 (00:30) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>Coffee</h3><p>11:00 (00:30) Foyer</p><p></p><h3>Git annex recipes</h3><p>11:30 (00:20) Event Hall</p><p>Timothy Sanders</p><details><summary>Abstract</summary><p>I have come up with many recipes over the years for scaling
git-annex repositories in terms of large numbers of keys,
large file sizes, and increased transfer efficiency.
I have working examples that I use internally that I can
demonstrate. (1) Second-order keys; using metadata to describe
keys that can be derived from other keys. I primarily used
this to help with the problem of too many keys referencing
small files. This is building off of the work of others, but I
believe I have made useful improvements, and I would like to
polish it up and share it.
One very early example is here:
https://github.com/unqueued/repo.macintoshgarden.org-fileset/
For now, I stripped out all but the location data from the
git-annex branch. Files smaller than 50M are contained in
second-order keys (8b0b0af8-5e76-449c-b0ae-766fcac0bc58). The
other uuids are for standard backends, including a Google
Drive account which has very strict limits on requests, and it
would have been very difficult to process over 10k keys
directly. There are also other cases where keys can be
reliably reproduced from other keys.
(2) Differential storage with git-annex using bup (or borg). I
built of off posts on the forums from years ago, and came up
with some really useful workflows for combining the benefits
of git-annex location tracking and branching with differential
compression. I have scripts used for automation, and some
example repos and case studies. For example, I have a repo
which contains file indexes that are over 60GiB, but only
consume about 6GiB, using bup packfiles. I can benefit from
differential compression over different time ranges, like per
year, or for the entire history, while minimizing storage
usage. I will publish a working example in the next few weeks,
but I have only used it internally for years.</p></details><h3>OpenNeuro and DataLad</h3><p>11:50 (00:20) Event Hall</p><p>Nell Hardcastle</p><details><summary>Abstract</summary><p>A history of OpenNeuro's adoption of DataLad and the evolution
of DataLad and git-annex support on the platform. In 2017
OpenNeuro was preparing to launch with the original data
backend implemented as block storage without git-annex. The
decision was made to move OpenNeuro to DataLad and a quick
prototype for this backend service was created and eventually
brought to production for the public release of OpenNeuro.
Since 2017 the platform has evolved to support many of the
unique advantages of DataLad datasets. This talk discusses the
architecture of OpenNeuro, some of the challenges encountered
using git-annex as the center of our application’s data model
in cloud environments, solutions developed, and future work to
improve upon OpenNeuro’s archival and distribution of DataLad
datasets.</p></details><h3>Questions and panel discussion</h3><p>12:10 (00:20) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>Lunch (self-organized, outside venue)</h3><p>12:30 (01:30) Foyer</p><p></p><h3>GIN-Tonic: (trying to) making datalad accessible to non-coders</h3><p>14:00 (00:20) Event Hall</p><p>Julien Colomb</p><details><summary>Abstract</summary><p>With the GIN-Tonic suite, we work toward a workflow where the
users only needs to use the browser and some synchronisation
scripts to manage their datalad repositories. Settings are
prepared in a template (including submodules) and a browser
extension (written in Go) create new repositories from that
template. I would report on successes and failures of the
strategy.</p></details><h3>Onedata as a Platform: Distributed Repository for Virtualizing Data and Long-term Preservation</h3><p>14:20 (00:20) Event Hall</p><p>Łukasz Dutka</p><details><summary>Abstract</summary><p>With the proliferation of digital data, reliable storage, easy
accessibility, and long-term preservation have become
paramount. Onedata, a novel platform, emerges as a solution
for these challenges by enabling a distributed repository
framework for virtualizing data. This presentation delves into
how Onedata facilitates seamless data management and ensures
long-term preservation. By virtualizing data, Onedata
abstracts the underlying storage infrastructures enabling a
unified view and easy sharing among different
stakeholders. Furthermore, its distributed repository nature
significantly enhances data durability and availability. The
in-built mechanisms for metadata management and data
replication ensure that the information remains intact and
accessible over extended periods. Through a detailed
exploration of its architecture and functionalities, this
presentation will highlight how Onedata can be a robust
platform for modern data management and long-term preservation
needs, catering to academia, industry, and beyond. The
insights provided will foster a better understanding of
leveraging distributed repository platforms in navigating the
complex landscape of digital data preservation.</p></details><h3>Questions and panel discussion</h3><p>14:40 (00:20) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>Coffee</h3><p>15:00 (00:30) Foyer</p><p></p><h3>Workflow provenance–based scheduling</h3><p>15:30 (00:20) Event Hall</p><p>Pedro Martinez</p><details><summary>Abstract</summary><p>Scientific computing workflows have become increasingly
complex, often comprising of numerous interdependent tasks
executed on distributed computing resources. Provenance data,
or the history of computational processes, provide a vital
link between data reproducibility and task
scheduling. Workflows with recorded data provenance can
seamlessly integrate with separate workflow management
systems, eliminating the need for inter-system communication.
In this talk, we introduce a novel tool to perform
provenance-based workflow scheduling. Our approach leverages
an abstract graph builder tool designed to create abstract
graphs representing the high-level structure of
workflows. These abstract graphs emphasize dependencies and
data flows, facilitating a better understanding of the
computational process. Concurrently, we extract concrete
graphs from workflow provenance data recorded with DataLad
that reflect the actual execution history.
The core of our approach lies in comparing the abstract graph
to concrete graphs produced by separate runs of the workflow
for a set of input parameters. By computing the difference we
can pinpoint tasks that remain unexecuted or require
re-execution due to errors or changes in input data and
automatically schedule these tasks. We will outline future
directions for this research, including potential extensions
to support system agnostic scheduling, and scalability
considerations.</p></details><h3>Optimisation in Network Engineering: Challenges and Solutions in Research Data Management</h3><p>15:50 (00:10) Event Hall</p><p>Julius Breuer</p><details><summary>Abstract</summary><p>In the complex realm of network engineering design,
optimisation methods have been instrumental, using a range of
components across different systems and scenarios. However,
this complexity presents a dual challenge: first, managing,
tracking and combining thousands of optimisation calculations,
including the specifics of component data, system
classifications, scenarios considered, and settings
applied. Second, integrating diverse data from multiple
sources that do not all reside in one place. Third, the
possibility of collaboration (in this case with students,
potentially with more people). Such challenges emphasise the
need for rigorous research data management. Questions such as
"which component data was used in which system?" or the
provenance of component data come to the fore. To answer
these questions, DataLad is used to store disparate data,
models, settings and results in an effective and distributed
manner. DataLad's provenance reduces the redundancy of storage
and the effort required for publication, while increasing
confidence in the results. This is done in the context of a
research project, but the same questions arise for the
industrial application of what has been researched.</p></details><h3>fMRI Pipelines on HPC with DataLad and ReproMan</h3><p>16:00 (00:10) Event Hall</p><p>Joe Wechsler</p><details><summary>Abstract</summary><p>In this lightning talk, I will share my experience using
DataLad, git-annex and ReproMan to run software pipelines on
hundreds of fMRI datasets on an HPC cluster. Potential
topics may include: (a) The use of ReproMan to avoid the
difficulties of using datalad containers-run in parallel on
an HPC. (b) How to use DataLad on a scratch filesystem that
periodically purges files to save space. (c) A simple
algorithm I implemented in ReproMan to prevent excess
runtime due to outliers in parallel jobs. (d) The pros and
cons of the YODA-BIDS layout for neuroimaging data. I hope
my talk will prompt discussion with those hoping to learn
more from my experience as well as those who have found
alternative solutions to similar challenges.</p></details><h3>Reproducibility vs. computational efficiency on HPC systems</h3><p>16:10 (00:10) Event Hall</p><p>Felix Hoffstaedter</p><details><summary>Abstract</summary><p>HPC systems have particular hard- and software configurations that
introduce specific challenges for the implementation of reproducible
data processing workflows. The DataLad based 'FAIRly big workflow'
allows for a separation of the compute environment from the
processing pipeline enabling automatic reproducibility over systems.
Yet, the sheer size of RAM and CPUs on HPC systems will allow for
different ways to optimize compute jobs in contrast to compute
clusters and certainly the average workstation/laptop. In this talk,
I discuss general differences between HCP and more standard compute
environments regarding necessary choices for the setup of processing
pipelines to be reproducible. Among the main factors are the
availability of RAM, local storage, inodes and wall clock time.</p></details><h3>Questions and panel discussion</h3><p>16:20 (00:40) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>End of day</h3><p>17:00 (00:30) Foyer</p><p></p><hr><h2 id=2024-04-05>2024-04-05</h2><h3>Coffee</h3><p>08:30 (00:30) Foyer</p><p></p><h3>Welcome and overview</h3><p>09:00 (00:15) Event Hall</p><p></p><details><summary>Abstract</summary><p>Welcome and overview: day 2.</p></details><h3>Neuroscientific data management using DataLad</h3><p>09:15 (00:20) Event Hall</p><p>Julian Kosciessa</p><details><summary>Abstract</summary><p>Robust data management from raw data to result publication
is necessary to make scientific research more widely
reusable. This remains a challenge, particularly in projects
that involve a variety of subcomponents and large data. In
this talk, I provide a user perspective on using DataLad
procedures for structuring, managing, and sharing complex
cognitive neuroscience projects. By showcasing example
multimodal neuroimaging projects that include e.g.,
electroencephalography (EEG), functional magnetic resonance
imaging (fMRI), and behavioral data, I will highlight
workflows that are uniquely enabled by the distributed
nature of DataLad. Based on my experiences, I will also
indicate remaining roadblocks I perceive to widespread
adoption.</p></details><h3>Staying in Control of your Scientific Data with Git Annex</h3><p>09:35 (00:20) Event Hall</p><p>Yann Büchau</p><details><summary>Abstract</summary><p>Scientific experiments can produce a lot of data, often very
different in kind and scattered across devices and even remote
locations. Keeping all of this in check is not a simple task
and failure to do so can easily cause data loss due to
accidental deletion or hardware failure (think cheap SD cards
in measurement devices at remote locations). Git Annex can
help with synchronisation, catalogisation, versioning and
archival of data as well as collaboration.</p></details><h3>Questions and panel discussion</h3><p>09:55 (00:20) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>Fusion of Multiple Open Source Applications for Data Management Workflows in Psychology and Neuroscience</h3><p>10:15 (00:20) Event Hall</p><p>Julia-Katharina Pfarr</p><details><summary>Abstract</summary><p>Finding a compromise between researchers’ needs, their skills
in data management, data access restrictions, and limited
funding for RDM is a complex but highly relevant and timely
challenge. At the University of Marburg this challenge is
taken on by the team of the “Data Hub”. The team consists of
several people with different responsibilities, backgrounds,
and affiliations such as project management staff, scientific
staff, data stewards, data scientists, technical
administrative staff, located in Marburg and Gießen. The Data
Hub is funded by The Adaptive Mind (TAM) and supported by the
information infrastructure project (NOWA) of the SFB135, which
are consortia in the fields of psychology and neuroscience,
with over 50 involved PIs, based in several locations in the
federal state of Hesse, Germany.
Although the research data in the two consortia are restricted
to the fields of psychology and neuroscience, a major
challenge is the need to harmonize heterogeneous data. The
data encompass research data from different modalities such as
behavior, eye tracking, EEG and neuroimaging as well as code
for experiments and analysis in various programming
languages. Therefore, the data management workflow needs to be
applicable to heterogeneous in- and output data, different
project sizes, and numbers of researchers
involved. Furthermore, tools need to be able to integrate
those heterogeneous data by utilizing a harmonizing standard
in the field (here: BIDS). To increase the reproducibility of
research findings, an integration of version control and
provenance tracking (here: DataLad) should be available.
For this, the team must have an understanding at which point
to include the researchers: How much background knowledge
about software do they have and how much do they really need?
Which functions of the software are necessary and which ones
can be skipped because they’ll never apply to the researchers’
work? Do they need a lot of hands-on practice or is the
concept enough?
In our presentation, we will first introduce the Data Hub of
the University of Marburg and its technical architecture. We
will then present the data management tools utilized in the
Data Hub, i.e., DataLad, GIN, GitLab, JupyterHub, and BIDS. We
will specifically focus on how these tools are interconnected,
i.e., the research data management workflow of the Data
Hub. Then, we will outline the challenges for both the
researchers as well as the Data Stewards regarding training,
support and maintenance of the services.</p></details><h3>Balancing Efficiency and Standardization for a Microscopic Image Repository on an HPC System</h3><p>10:35 (00:20) Event Hall</p><p>Julia Thönnißen</p><details><summary>Abstract</summary><p>Understanding the human brain is one of the greatest
challenges of modern science. In order to study its complex
structural and functional organization, data from different
modalities and resolutions must be linked together. This
requires scalable and reproducible workflows ranging from the
extraction of multimodal data from different repositories to
AI-driven analysis and visualization [1]. One fundamental
challenge therein is to store and organize big image datasets
in appropriate repositories. Here we address the case of
building a repository of high-resolution microscopy scans for
whole human brain sections in the order of multiple Petabytes
[1]. Since data duplication is prohibitive for such volumes,
images need to be stored in a way that follows community
standards, supports provenance tracking, and meets performance
requirements of high-throughput ingestion, highly parallel
processing on HPC systems, as well as ad-hoc random access for
interactive visualization.
To digitize an entire human brain, high-throughput scanners
need to capture over 7000 histological brain sections. During
this process, a scanner acquires a z-stack, which consists of
30 TIFF images per tissue section, each representing a
different focus level. The images are automatically
transferred from the scanner to a gateway server, where they
are pre-organised into subfolders per brain section for
detailed automated quality control (QC). Once a z-stack passes
QC, it is transferred to the parallel file system (GPFS) on
the supercomputer via NFS-mount. For one human brain, this
results in 7000 folders with about 2 PByte of image data in
about 20K files in total. From there, the data are accessed
simultaneously by different applications and pipelines with
their very heterogeneous requirements. HPC analyses based on
Deep Learning such as cell segmentation or brain mapping rely
on fast random access and parallel I/O to stream image patches
efficiently to GPUs. Remote visualization and annotation on
the other hand requires exposure of the data through an HTTP
service on a VM, with access to higher capacity storage to
serve different data at the same time. These demands can be
covered by multi-tier HPC storage, which provides dedicated
partitions. The High Performance Storage Tier offers low
latency and high bandwidth for analysis, while the Extended
Capacity Storage Tier is capacity-optimized with a lower
latency, meeting the needs for visualization. Exposing the
data on different tiers requires controlled staging and
unstaging. We organize the image data folders via DataLad
datasets, which allows well defined staging across these
partitions for different applications, ensures that all data
is tracked and versioned from distributed storage throughout
the workflow, and enables provenance tracking. To reduce the
number of files in one DataLad repository, each section folder
has been designed as a subdataset of a superdataset that
contains all section folders.
The current approach to managing data has two
deficiencies. Firstly, the TIFF format is not optimized for
HPC usage due to the lack of parallel I/O support, resulting
in data duplication due to conversion to HDF5. Secondly, the
current data organization is not compatible with upcoming
community standards, complicating collaborative
efforts. Therefore, standardization of the file format and
folder structure is a major objective for the near future. The
widely accepted community standard for organizing neuroscience
data is the Brain Imaging Data Structure (BIDS). Its extension
for microscopy proposes splitting the data into subjects and
samples, while using either (OME-)TIFF or OME-ZARR as a file
format. Particularly, the NGFF file format OME-ZARR appears to
be the suitable choice for the workflow described, as it is
more performant on HPC and cloud compatible as opposed to
TIFF. However, restructuring the current data layout is a
complex task. Adopting the BIDS standard results in large
amounts of inodes and files because (1) multiple folders and
sidecar files are created and (2) OME-ZARR files are comprised
of many small files. DataLad annex undergoes expansion with
the increase in the number of files leading to high inode
usage and reduced performance. An effective solution to this
problem may involve the optimization of the size of DataLad
subdatasets. However, the key consideration is that GPFS file
systems enforce a limit on the number of inodes, which cannot
be surpassed. This raises the following questions: How can
usage of inodes be minimized while adhering to BIDS and
utilizing DataLad? Should performant file formats with
minimal inode usage, such as ZARR v3 or HDF5, be incorporated
into the BIDS standard? What is a good balance for DataLad
subdataset sizes? Discussions with the community may provide
valuable perspectives for advancing this issue.
[1] Amunts K, Lippert T. Brain research challenges
supercomputing. Science 374, 1054-1055
(2021). DOI:10.1126/science.abl8519</p></details><h3>DataLad-Registry: Bringing Benefits of Centrality to DataLad</h3><p>10:55 (00:20) Event Hall</p><p>Isaac To, Yaroslav Halchenko</p><details><summary>Abstract</summary><p>DataLad-Registry is a service that maintains up-to-date information on over ten
thousand datasets, with the collection expanding as more datasets are added.
This talk will explore how DataLad-Registry automatically registers datasets from
the internet, extracts metadata from them, and keeps these datasets and their
corresponding metadata up-to-date. We'll showcase the datasets and metadata
types currently available within DataLad-Registry and demonstrate the service's
search capability. Additionally, we'll provide an overview of the API and reveal
the underlying service components of DataLad-Registry. The presentation will
conclude with a discussion on ongoing and future developments, inviting audience
input to shape the future of DataLad-Registry.</p></details><h3>Questions and panel discussion</h3><p>11:15 (00:45) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>Lunch (self-organized, outside venue)</h3><p>12:00 (01:30) Foyer</p><p></p><h3>Reproducible and replicable data science in the presence of patient confidentiality concerns by utilizing git-annex and the Data Science Orchestrator</h3><p>13:50 (00:40) Event Hall</p><p>Markus Katharina Brechtel, Philipp Kaluza</p><details><summary>Abstract</summary><p>Health-related data for patients is among the most sensitive
data when it comes to data privacy concerns. Data science
projects in the medical domain must thus pass a very high bar
before allowing data researchers access to potentially
personally identifiable data, or pseudonymized patient data
that carries an inherent risk of depseudonymization.
In the project "Data Science Orchestrator", we propose an
organizational framework for ethically chaperoning and
risk-managing such projects while they are under way, and a
software stack that helps in this task. At the same time this
software stack will provide an audit trail across the project
that is verifyable even by external scientists without access
to the raw data, while keeping the option for future
reproducibility studies and replicability studies open. This
is achieved by utilizing git-annex and datalad in a novel way
to provide partial data blinding.
Because collecting study-relevant data is often a time- and
labor-intensive undertaking in the medical domain, many
projects are undertaken by associations that span multiple
hospitals, administrative domains, and often even multiple
states. Therefore the "Data Science Orchestrator" project also
implements distributed data science computations, which allow
to honor these existing administrative boundaries by means of
a federated access model, all while keeping the most sensitive
data in-house and exclusively in a tightly controlled
computation environment.
This work was sponsored by Deutsche Zentren für
Gesundheitsforschung (DZG) and BMBF.</p></details><h3>A Tour of Magit</h3><p>14:30 (00:20) Event Hall</p><p>Kyle Meyer</p><details><summary>Abstract</summary><p>Magit is an Emacs interface to Git. Through it, you can drive Git
operations, even advanced ones, by typing short key sequences. This
talk will show Magit in action. I will give a general overview and
then highlight features for preparing and refining a series of
commits.</p></details><h3>Questions and panel discussion</h3><p>14:50 (00:20) Event Hall</p><p></p><details><summary>Abstract</summary><p>Questions and panel discussion</p></details><h3>Coffee</h3><p>15:10 (00:30) Foyer</p><p></p><h3>Distributed Metadata and Data with Dataverse</h3><p>15:40 (00:40) Event Hall</p><p>Philip Durbin, Jan Range, Oliver Bertuch</p><details><summary>Abstract</summary><p>Dataverse is open source research data repository software that has
supported distributed metadata for a long time and is increasingly
supporting distributed data. Learn about the Open Archives Initiative
Protocol for Metadata Harvesting (OAI-PMH) and file "stores" within
Dataverse that can be hosted locally, on S3, Globus, or remote
locations. We plan to demonstrate a proof of concept for a distributed
storage configuration in Jülich DATA and the use of Dataverse APIs to
manage data with Python.</p></details><h3>Unconference</h3><p>16:20 (00:40) Event Hall</p><p></p><details><summary>Abstract</summary><p>Unconference slot</p></details><h3>Dinner and social (self-organized, outside venue)</h3><p>17:00 (01:00) Foyer</p><p></p><hr><h2 id=2024-04-06>2024-04-06</h2><h3>Coffee</h3><p>08:30 (00:30) Seminar room 1, 3rd floor</p><p></p><h3>Kick off / Pitches</h3><p>09:00 (00:30) Seminar room 1, 3rd floor</p><p></p><h3>Hacking</h3><p>09:30 (02:30) Seminar room 1, 3rd floor</p><p></p><h3>Lunch (self-organized, outside venue)</h3><p>12:00 (01:30) Seminar room 1, 3rd floor</p><p></p><h3>Hacking</h3><p>13:30 (01:00) Seminar room 1, 3rd floor</p><p></p><h3>Coffee</h3><p>14:30 (00:15) Seminar room 1, 3rd floor</p><p></p><h3>Hacking</h3><p>14:45 (01:45) Seminar room 1, 3rd floor</p><p></p><h3>Wrap-up</h3><p>16:30 (00:30) Seminar room 1, 3rd floor</p><p></p><h3>Dinner and social (self-organized, outside venue)</h3><p>17:00 (02:00) Seminar room 1, 3rd floor</p><p></p><hr></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://distribits.live>distribits</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>